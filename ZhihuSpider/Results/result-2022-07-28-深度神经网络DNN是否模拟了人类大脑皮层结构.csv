question_title,answer_url,author_name,fans_count,created_time,updated_time,comment_count,voteup_count,content
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/184888043,知乎用户,8316,2017-06-16T16:11:11.000Z,2017-07-24T05:13:43.000Z,109,1840,"看到目前大部分回答的作者应该都是CS 领域的，我自己是 生物本科，认知神经科学研究生在读，课余时间比较喜欢编程和机器学习，正在自学，了解的稍微多一些。我试着从我的角度来说下我看到的 深度学习和 神经科学的联系。深度学习和神经科学这两个学科现在都很大，我的经历尚浅，如果大家发现哪里说得不太对，欢迎提出指正，谢谢！那我们就自底往上说。神经元在深度学习领域，神经元是最底层的单元，如果用感知机的模型， wx + b, 加上一个激活函数构成了全部，输入和输出都是数字，研究的比较清楚，别的不说，在参数已知的情况下，有了输入可以计算输出，有了输出可以计算输入。但在神经科学领域，神经元并不是最底层的单位，举例来说，有人在做神经元膜离子通道相关的工作。一个神经元的输入，可以分为三部分，从其他神经元来的电信号输入，化学信号输入，还有编码在细胞内的信号（兴奋，抑制类型，这里可以类比为 激活函数？），输出也是三个，电输出，化学输出，改变自身状态（LTP 长时程增强, LTD长时程抑制）。我们是否足够了解神经元？ 我个人十分怀疑这一点，前几天还看到一个关于神经元的进展，大意是神经元不仅能对单一信号产生反应。。还能对一定一定间隔的信号产生反应。。 神经元的底层编码能力其实更强。。。我们神经科学发展了这么久，可能真的连神经元都没真正的搞清楚。在这另外说一句。 深度神经网络里面，大部分节点都是等同的，但是在人类神经网络里面，并不是这样，不同的脑区，甚至脑区内部，神经元的形态都可以有很大的差异，如V1内部的六层就是基于神经元形态的区分。从这个角度，人类神经系统要更复杂一些。我个人并不否认每一种神经元可以用不同初始化参数的 节点来代替，但是目前来说，复杂度还是要比深度神经网络要高。信号编码方式再说编码方式，神经科学里面的 神经元是会产生0-1 的动作电位，通过动作电位的频率来编码相应的信号（脑子里面的大部分是这样，外周会有其他形式的），而人工神经网络？大部分我们听到的，看到的应该都不是这种方式编码的，但是 脉冲神经网络 这个东西确实也有，（今天去ASSC 开会的时候看到了一个很有趣的工作，在评论区简单说了下，有兴趣可以去看。）神经网络的结构目前的深度神经网络主要是三种结构， DNN（全连接的），CNN（卷积）， RNN（循环）。还有一些很奇怪的， 比如说。。。Attention 的？不好意思，文章还没看，不敢乱说。。。放点图：DNN:来自 ： Neural Networks - UfldlCNN：出处： AlexNetRNN：出处： Understanding LSTM Networks神经科学里面的网络结构，此处以V1 为例：来源： Adaptation and Neuronal Network in Visual Cortex感谢@滕建超提供新的图片，比我之前那个强多了，这张图表达分层结构表达的更好一些。来源： Neocortical layer 6, a review和大家想的不同，视觉区分了V1，V2，V3，V4，V5（MT），上面还有FFA， 和一些掌管更高级功能的脑区。在这里面每一个小的视皮层里面，并不是纯由神经元互相连接构成的，仍然存在不同的层级结构。这里去google 找了一张图，不用管具体的文章，主要说明的是V1 的精细结构和连接关系。V1 的主要功能是 识别点和不同角度的线段（Hubel 和W 在上世纪50年代在猫上的工作），但是其实不止如此，V1 还对颜色有一定的感知。如果在这个层面作比较，我自己的理解是， 人类神经网络是 DNN+ CNN + RNN 再加上脉冲作为编码方式。层内更像DNN， 层间和CNN 很类似，在时间上展开就是RNN。好，我们继续。训练方式：深度神经网络的训练方式主要是 反向传播，从输出层一直反向传播到第一层，每一层不断修正出现的错误。但是大脑里面并没有类似反向传播机制，最简单的解释，神经元信号传递具有方向性，并没机会把信号返回上一层。举个例子，我要拿起手边的杯子，视觉发现向右偏移了一点，那我会自然而然的移动整个手臂向左一点，然后试着去重新抓住杯子。好像没人是让手指，手，最后是手臂朝杯子移动，甚至多次才能最后成功吧。在此引用下一篇文章里面的图。来源文章： Towards Biologically Plausible Error Signal Transmission in Neural Networks我们的大脑，更像最后 DFA 的原理。出错了，把误差送到一个更靠近输入的地方，然后重新训练。记忆和遗忘：提到记忆的话，这里主要说的是LSTM， LSTM 的记忆储存在每个节点的权重里面，同时有专门的 遗忘门 控制遗忘速率。这些都是以数字的形式存储的。在神经系统里面，记忆的存储是由一些脑区的突触的形成和消失来存储的。其实他们有一个比较共通的地方在于，他们在训练过程中都是渐变的。得益于反向传播机制和 神经系统的生物性，他们在训练过程中和在不断的学习过程中都只能以一个相对慢的速度发生改变，从学习速率角度来讲，他们是比较相似的。然后我们来说遗忘。遗忘在LSTM 里面是通过门来控制的，在神经系统里面，我觉得是和STDP相关的，它的基础是 Hebb 假说， Fire Together， Wire Together， 同步放电的神经元倾向于建立一个更强的连接。STDP 拓展了这一点，考虑了两神经元放电的先后顺序带来的影响。来源：Synaptic Modification by Correlated Activity: Hebb&amp;amp;amp;amp;amp;amp;#x27;s Postulate Revisited简单来说，如果突触前神经元放电先于突触后神经元（神经元信号传导具有方向性，从突触前到突触后），这个突触会进入一个LTP 长时程增强状态，会对来自突触前的信号有更强的反应。反之，如果突触前神经元放电后于突触后，则会进入一个长时程抑制的状态（说明他俩并没有接收到相同来源的信号，信号不相关），一段时间的反应更弱。深度神经网络里面门的权重也是 反向传播训练出来的，也有渐变的这个性质，当对于快速变化的刺激，有一定的滞后。从这个角度来说，人类神经系统要更灵活一些，可以在很短的时间内完成状态的切换。觉得想说的大概就是这些，因为我自己做的研究是 视觉注意，更多在人身上做，所以对于中间的环路级别的研究，并不是特别的熟悉。再往上，谈到人类大脑皮层的工作，个人觉得做的十分的有限，对于大部分脑区，我们并不知道他们是怎么工作的，只是能把不同的脑区和不同的功能对应起来（还不一定准。。）。在这个角度上谈他们的异同是不太负责的。。。容易被打脸。接下来我会试着邀请几个朋友来说下环路这个级别的事情。。然后会找其他同行帮我挑错和补充。。。。。很多东西都是按照记忆写的。。一些东西不一定准确。。最后说下自己的观点吧正如在提纲里面提到的。 对的答案往往类似，而错误的答案各有不同。地球上这么多高等的生命都有类似的底层网络结构，而其中的一种还发展出了这么伟大的文明，神经网络这个结构，至少已经被我们自己证明是一种有效的形式。但是是不是智能这个形式的全局最优解？我个人持怀疑态度。神经网络是一个有效的结构，所以大家用这个结构做出一些很好的结果，我一定都不吃惊。但是如果谈模拟的话，就是尽力要往这个方向靠。这点上，我个人并不是十分看好这种方式。我们向蝙蝠学习用声音定位，发展的声呐无论是距离还是效果都远超蝙蝠。我们能超过蝙蝠的原因，第一是我们的技术有拓展性，底层原理共通的情况下，解决工程和机械问题，我们可以不那么轻松但是也做到了探测几千米，甚至几十公里。第二个原因就是我们需要而蝙蝠不需要，他们天天在山洞里面睡觉。。哪用得着探测几十公里的距离，探到了也吃不着。。其实人类大脑也很类似，大脑是一个进化的产物。是由环境不断塑造而成的，人为什么没进化出计算机一样的计算能力，因为不需要。但是其实反过来也有一定的共通的地方，大脑里面的一些东西，我们也不需要，我们千百年来忍饥挨饿进化出的 对于脂肪摄入的需求，在儿童时期对于糖类摄取的需求。这么说的话，我们对于大脑，同样去其糟粕，取其精华不是更好吗？我上面提到的是一个理想的情况，我们对大脑已经了解的比较透彻的，知道该去掉哪，留下哪。。但是现在。。。可能还要走一段模拟的路子。。。。大概就是这个观点。 总结一下，就是， 深度神经网络和大脑皮层有共通的地方，但是并不能算是模拟。只是大家都找到了解题的同一个思路而已。感谢阅读，希望大家多提宝贵意见。如果您都读到这了，欢迎去看看评论区，有很多不错的讨论。Harold_Yue 第一版草稿写于 2017.6.16 0:10Harold_Yue 第二版草稿写于 2017.6.17 11:10 加入了 训练方式， 记忆和遗忘，以及自己的观点 三部分。Harold_Yue 于 2017.6.19 12:54 加了张图，另外表示评论区很赞。拓展阅读以及参考文献：从科研到脑科学 - 知乎专栏Understanding LSTM NetworksAlexNet： ImageNet Classification with Deep Convolutional Neural NetworksTowards Biologically Plausible Error Signal Transmission in Neural NetworksSTDP: Synaptic Modification by Correlated Activity: Hebb&amp;amp;amp;amp;amp;amp;#x27;s Postulate Revisited神经元对一定间隔信号反应： Learned response sequences in cerebellar Purkinje cells新智元相关报道 ： 【新智元专访】神经元本身也能编程，神经网络学习能力远超预期"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/540640047,周博磊,100129,2018-11-28T02:09:07.000Z,2018-11-28T02:09:07.000Z,3,189,"这个问题涉及神经科学和AI的关联。我推荐一篇由Deepmind的CEO主笔的发表在Neuron杂志上的综述：Neuroscience-Inspired Artificial Intelligencewww.columbia.edu/cu/appliedneuroshp/Papers/out.pdf文章综述了神经科学和人工智能的发展历史，开门见山陈述了下面两个很有意思的观点：Neuroscience provides a rich source of inspiration for new types of algorithms and architectures, independent of and complementary to the mathematical and logic-based methods an d ideas that have largely dominated traditional approaches to AI.Neuroscience can provide validation of AI techniques that already exist. If a known algorithm is subsequently found to be implemented in the brain, then that is strong support for its plausibility as an integral component of an overall general intelligence system.Inspiration和validation我觉得是对目前AI系统非常重要的两点补充。神经科学家在研究大脑这个计算系统时，会采取更直接，或者在数学建模者看来更naive的办法去研究，因为这个由千万年自然自下而上进化而来的蛋白质计算系统太复杂了，没法用数学或者物理理论去解释。而像AlphaGo这种Superhuaman AI系统，以后会越来越多。这样的系统已经很难用人类已知的理论去解释，所以从神经科学中挖掘相应的研究方法，也许确实是条路子。另外一方面，神经科学中的一些现象和结论，能佐证和解释AI算法的有效性。DeepMind一直践行这种bottom-up的研究方法，做出了非常优秀的研究工作。这在DeepMind的另外一篇Nature论文：Vector-based navigation using grid-like representations in artificial agents ，体现得淋漓尽致。在这篇论文里，他们发现了一些类似于Grid cell激活特性的神经元会在训练导航的artificial agent的内部表征里出现。而这个grid cells本身的研究，获得了2014年诺贝尔生理学奖for their discoveries of cells that constitute a positioning system in the brain。DeepMind的这篇论文，还找来了这个诺贝尔奖获得者来写序，也是牛逼炸了。所以我觉得，AI系统并不用强调自己的biological plausibility，因为毕竟工程上面能work是最好的。但是神经科学能给予的Inspiration和validation，给研究和构建更好的AI系统提供了支持。"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174198192,SIY.Z,18782,2017-05-25T01:29:40.000Z,2017-05-25T02:35:54.000Z,37,230,只能说一部分是。 1. 一开始提出时，显然是借鉴人脑的，权值更新规则是著名的生物学上的Hebb's rule(经常一起激活的神经元连接会加强)。但是，Hebb's rule 不能训练多层网络，实际上训练深度网络的是BP算法，而BP算法在生物学上很牵强(虽然一直有人argue说存在，但是并没有真正让人信服的结论) 2. LeCun在提CNN的时候提到了借鉴人脑。人脑视觉处理确实是分层的，并且非常重要的是CNN产生了和人脑非常一致的激活模式。但是，人脑处理公认一般也就4到5层(初级视皮层V1-V5)，但是你看次现在效果奇好的Residual，可以上千层。。。 3. 非常重要的一点，人脑神经元传递信号是脉冲形式的，有“时序和频率”的概念，而不是像神经网络一样不更新就是定值。脉冲形式也是能耗低和鲁棒性高的重要保证。 4. 强化学习被认为和人脑的多巴胺系统有非常重要的联系。多巴胺系统似乎能够传递强化学习中的td信号。实验有初步认证，但是仍待观察。但是提问中问的是大脑皮层Orz. 5. 神经元相比神经网络中的人工神经元复杂的多，神经元有很多“内态”，多种效用不同的神经递质和激活模式。这点上看两者又是差异很大的。 6. 一些边缘的类似，比如说注意力模型等。这些可能有关系（比如人脑确实是使用神经元以和DNN类似的模式来控制注意力的），但目前说不清。暂时想到这些区别和联系，如果找到更多的之后会补上不过我还是想补一些额外的话，题者虽然只是问DNN是否模拟了人脑结构，但是我隐约还是感到有些倾向性，虽然不是所有人都这么想，但是肯定是有人有这种看法的。DNN和人脑都有固有的缺陷。如果将来有证据说明，人脑一点都不“深”，并不意味着DNN有问题。如果人脑真的有几千层的结构，那么受生物学的限制（神经元工作频率很低的），人的反应会极度缓慢，根本不适合生存，所以说即使深能提高智商，自然进化也避免如此；而DNN没有这种问题。同样的，使用脉冲信号也是不得已的方法，否则整体功耗和稳定性会非常糟（想象一下数亿的神经元同时都在维持连接（这还是非常稀疏的情况下），就像神经网络一样，如果不受干扰就要保持相对强的连接强度，这功耗真的把生物搞死）。早期的一些小生物（轮虫还是什么的记不清了），确实是不使用脉冲的策略，但是大了之后很难说使用脉冲不是一种无奈之举。我个人不相信脉冲可以传递精度很高的信号，这样以来很多类似BP的更新策略难以实际运行，即使它能够提高大脑的处理能力，如果真的如此反而DNN获得了优势。但是脉冲又带来了时序的概念，这又是一件焉知祸福的东西，控制好的时序可以高效构成某种模式，但是控制不好又显然造成了异步更新的麻烦。所以说设计DNN也不能迷信人脑，实际中人脑可以作为一个很好的启发式和验证方式，但是收到各种生物学限制的系统几乎必然有大量的缺陷，所以DNN做的如何最终还是看实际效果，而不是和人脑类比。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/185530107,知乎用户,101210,2017-06-18T07:49:18.000Z,2017-07-30T21:28:55.000Z,25,15,"Cell今年五月才出的一篇文章，显示了猕猴的大脑识别不同脸部图像的机制和CNN极其相似，而且激励信号和输入信号的线性组合成正比，也和CNN中的神经元类似。而且，一共就只需要那么两百个神经元…The Code for Facial Identity in the Primate Brainauthor Le Chang, Doris Y. TsaoHighlights•Facial images can be linearly reconstructed using responses of ∼200 face cells•Face cells display flat tuning along dimensions orthogonal to the axis being coded•The axis model is more efficient, robust, and flexible than the exemplar model•Face patches ML/MF and AM carry complementary information about facesSummaryPrimates recognize complex objects such as faces with remarkable speed and reliability. Here, we reveal the brain’s code for facial identity. Experiments in macaques demonstrate an extraordinarily simple transformation between faces and responses of cells in face patches. By formatting faces as points in a high-dimensional linear space, we discovered that each face cell’s firing rate is proportional to the projection of an incoming face stimulus onto a single axis in this space, allowing a face cell ensemble to encode the location of any face in the space. Using this code, we could precisely decode faces from neural population responses and predict neural firing rates to faces. Furthermore, this code disavows the long-standing assumption that face cells encode specific facial identities, confirmed by engineering faces with drastically different appearance that elicited identical responses in single face cells. Our work suggests that other objects could be encoded by analogous metric coordinate systems.http://www.cell.com/cell/fulltext/S0092-8674(17)30538-X?_returnURL=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS009286741730538X%3Fshowall%3Dtrue"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/213490854,知乎用户,15854,2017-08-13T05:28:48.000Z,2017-08-13T05:28:49.000Z,2,11,个人认为 两者联系还是很多的但也有很多差别。不过DNN更多地模拟了人类神经系统的感觉系统部分，像CNN的灵感基本就是从视觉系统来的，大多数DNN（除外RNN）完成的是最基本的特征表示这块。至于得到的特征下一步怎么用？现在的DNN多数就接了个分类器，这是很不像人脑的。人脑在把各种途径得到的特征都送入一个叫丘脑的地方进行整合反馈。而丘脑如何工作已经不是很清楚了。即使这样丘脑也形成不了智能，丘脑又叫原始脑只能进行比条件反射略高级点的行为。大脑皮层则是产生智慧的地方。在大脑皮层有许多皮质柱，每一个皮质柱相当于存储了一类知识的循环神经网络。大脑智能的决策基本不属于目前DNN探讨的内容，而有点像强化学习的知识学习。另外 DNN中的梯度反向传播和人脑的神经元联系目前看不出来，至于人类的神经系统其神经元的权重调节是否是决定性的、是否是后天学习调整的还是dna遗传带来的这些还不好说。不管怎么说DNN试图在模拟人类神经系统的部分工作机制，两者有着千丝万缕的联系。同时个人认为目前的DNN提供的几大类基本的网络结构已经比较成熟了，今后的方向可能更多的要转为搭建基于基本网络的复合网络、探究网络如何表示知识。学习不是简单的特征检测，到底是调整权重带来的智能还是复杂的复合网络的架构产生了智能都是需要探讨的。有可能CS目前研究的DNN最终也能找到一条通往强人工智能的解决方案，那很有可能与人类神经系统存在本质上相同的地方。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174175450,叛逆者,190156,2017-05-25T00:24:31.000Z,2017-05-25T00:24:32.000Z,8,53,是有密切关系的。90年代发展起来的大脑发育理论表明，大脑中的神经元组成了不同的层次，这些层次相互连接，形成一个过滤体系。在这些层次中，每层神经元在其所处的环境中获取一部分信息，经过处理后向更深的层级传递。这和DNN很相似。还有理论指出婴儿大脑发育就是那样一个区域一个区域进行的，每一各区域先自组织，然后再相连，按照一定先后顺序发育成整个大脑。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/185300186,Lanzhe Guo,1517,2017-06-17T16:04:28.000Z,2017-06-17T16:04:29.000Z,2,2,只是数学函数的多层嵌套而已
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174650934,知乎用户,58,2017-05-26T02:05:35.000Z,2017-05-26T02:07:11.000Z,1,25,"IEEE Spectrum采访伯克利大学机器学习专家 Michael Jordan时他的回答:I wouldn’t want to put labels on people and say that all computer scientists work one way, or all neuroscientists work another way. But it’s true that with neuroscience, it’s going to require decades or even hundreds of years to understand the deep principles. There is progress at the very lowest levels of neuroscience. But for issues of higher cognition—how we perceive, how we remember, how we act—we have no idea how neurons are storing information, how they are computing, what the rules are, what the algorithms are, what the representations are, and the like. So we are not yet in an era in which we can be using an understanding of the brain to guide us in the construction of intelligent systems.我不直译，他的意思大致是我们现今对神经科学的认识还远远不够。我们还不知道人类是如何理解和记忆事物，更不知道大脑是怎么存储信息和怎么运算的。我们现在只是知道人脑里有神经元以及它的组成和特征，然后我们根据神经元理论建立了神经网络算法，但这离真正的脑系统还差得远呢。归根结底，就是一句话，神经科学领域的科学家不给力啊。我们都不知道大脑是怎么运作的，怎么模仿呢。参考文献：Machine-Learning Maestro Michael Jordan on the Delusions of Big Data and Other Huge Engineering Efforts"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/177520498,gao sky,47,2017-06-01T15:37:19.000Z,2017-06-18T16:06:14.000Z,5,3,"你脑子需要自编码的逐层训练吗？你脑子会出现梯度消失吗？你脑子看东西是一个个滤波器筛选的吗？你脑子使用一堆数字连起来的吗？你还会自己学习的时候偶尔关闭一下脑细胞来提高学习效果吗？————————————————————————————————分割线，回答一下@UFO的问题，稀疏化和随机关闭有什么区别。。。区别大了。。。首先，None和0有木有区别，肯定有，好吧。。。这就是稀疏化和随即关闭的区别。想想一个2,2,1的神经网络，当网络输入为【1,1】的时候，不妨将w1设为【1,1；1，-1】，这个时候，正常情况下你的中间层h应该是【1,0】首先，这是一个稀疏化的表达，当我们再次引入dropout的时候，很有可能原本是1的那个神经元也变成了0.好吧，这个时候你的网络gg了，因为0出现了梯度消失。好吧。。。也许你会说，这是个少数情况，当我们的神经元数很多的时候，几乎不可能。那你就要回头想一想链式法则的权值调整过程了。首先，dropout是网络中拿掉了这些神经元，虽然表面上看相当于是置零，但是实际上相当于不存在，也就是说，当我们再去调整权值的时候，是不会修改这些权值的。这也就是dropout本身也可以看做一个boost的原因，他相当于集成了多个不同神经元组成的神经网络。而对于稀疏表达来说，即便当前的值是0，代表了你对别人不产生直接影响，可是这并不代表别人对你不产生影响啊。所以虽然，0无法传递，但是这个神经元依然还在，就像0和None的区别。最后，也是最最重要的，你的稀疏化来自什么？惩罚项。你真的可以通过一个简单的惩罚项，就可以求解这么复杂的优化问题，获得一个稀疏解吗？不信你去打印一个中间变量看看，如果你的输入不是稀疏的，那么你的中间层也不可能很准确的是0，通常情况下只能是一个很接近0的数字。当然，这里并不排除你采用了ReLU这种可以强制置零的激活函数。那么问题就来了，很小的数，是不是数，会不会对权值产生影响，答案是肯定的。所以，稀疏化和随机关闭的原则差别还是蛮大的。——————————————不知道这样解释可以不，大佬们轻喷"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/2198513320,倪静风,26887,2021-10-31T04:26:10.000Z,2021-10-31T04:31:30.000Z,0,4,不是的仙女蜂靠几百个神经元就可以飞行（另外几千个神经元没有核，有核的神经元只有几百个）仙女蜂_百度百科baike.baidu.com/item/%E4%BB%99%E5%A5%B3%E8%9C%82单细胞生物如草履虫不需要神经元，一样可以进食，敌我识别，捕食，对光电磁产生反应草履虫_百度百科baike.baidu.com/item/%E8%8D%89%E5%B1%A5%E8%99%AB比细胞小得多的病毒，人类至今研究不透。。。生物病毒_百度百科baike.baidu.com/item/%E7%94%9F%E7%89%A9%E7%97%85%E6%AF%92好多书上写的内容只是一种猜想而已，生物的复杂性远超想象
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174369414,小小帝奇,15,2017-05-25T08:05:07.000Z,2017-05-25T08:05:07.000Z,4,5,我就拿CNN这个网络来说吧。卷积神经网络出自上个世纪60年代科学家提出的感受野，当时科学家发现猫的每一个视觉神经元只会处理小块区域的视觉图像，CNN的卷积核就是参考感受野。80年代日本科学家提出神经认知神经元，它包含两类神经元，用来抽取特征的S-cells和用来扛形变的C-cells。S-cells对应于卷积核滤波，C-cells对应激活函数、最大池化。Alex参加ImageNet论文中的LRN层也是模仿了生物神经系统的侧抑制机制。我学习DL不到一个月，认知程度还不够。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174179249,匿名用户,0,2017-05-25T00:38:42.000Z,2017-05-25T00:38:42.000Z,34,61,没有，深度学习、神经网络等称呼仅仅是为了蹭热点，它的实质是堆砌简单的数学算子来构造一个复杂的函数系统。而这个函数是如此的复杂，使得人们难以用常规的数学工具去理论证明这个函数系统的性质，因此才会有人用脑神经经科学去解释它。即便如此，该领域的最新研究已经完全找不到人脑中的对应参照物，如残差网络ResNet的跳层结构。不过，随着各位数学大神的加入，一些简单的网络结构的理论证明已经初见苗头，相信在未来的某个时间点上，人们终归可以找到一个漂亮又不失朴素的数学方法来论证深度学习的有效性，进而指导该领域的学术走向。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/175176294,Genome,107,2017-05-27T05:23:49.000Z,2017-06-11T23:24:05.000Z,1,4,什么是模拟？我完成了一件事，你用相同的方式完成了，是模拟吗？那我用不同的方式完成了同一件事了？DNN的潜力我们只看到了冰山一角，同样大脑怎么工作的我们也只看到了冰山一角，基于这冰山一角去判断问题肯定有局限性。如果两种不同的方法都能完成同一个目标，那么他们内部必然有本质相同的东西（甚至是本质相同的），如鸟飞和飞机飞背后都是空气动力学的本质。同样DNN可以和大脑类比，只是现在的DNN和大脑的很多问题我们还说不清楚而已，而且毋庸置疑DNN一定会变得更强大。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174609485,baskice,84545,2017-05-26T00:06:39.000Z,2017-05-26T00:06:40.000Z,3,35,深度神经网络（DNN）是否模拟了人类大脑皮层结构？没有，不仅人类大脑皮层这么复杂的结构没有模仿到，连最简单的几种模式生物（比如秀丽隐杆线虫）复杂程度的千分之一都不及。你可以说 深度神经网络 这个方法（算法）创造时受到了神经学研究的启发，不能说这个方法是在模拟神经活动，更别提模拟人类大脑了。DNN的核心思路在于输入内容经过多层“感知层”处理后生成结果，再用 误差反向传播 算法进行优化训练。这么搞直接倾向就是中间感知层层数越多，结果越准确。这跟人脑，甚至不说人脑，就连模式生物的简单神经活动都没什么关系。神经处理并不注重层数，而更注重频率，时间，化学环境调节，甚至信号来源方向都会有影响。神经内的分子信号还经常影响自己的行为。循环神经网络Recurrent Neural Networks 中每个节点输出内容会影响自己这一思路，是借鉴现实神经元的行为。因此可以说 RNN相比DNN更加“模拟”神经行为。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/175001160,陈默,13361,2017-05-26T16:48:53.000Z,2017-05-26T16:48:53.000Z,3,21,最开始时候ANN是蹭了神经学的热点。为什么，生物学神经学什么的容易发nature science啊，计算机程序搭上大脑的噱头看起来多么的炫酷。这就是为啥当初ANN那么不work还火了这么多年，容易灌水啊。后来DNN居然给调work了，神经学又蹦出来蹭热点了：你看都是我们神经学的启发你们才搞出这么个东西。说到底，其实俩东西的p关系没有。强说有关系的话，那么跟飞机和飞鸡的关系也差不多吧。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/175011061,知乎用户,36,2017-05-26T17:31:02.000Z,2017-05-26T17:31:02.000Z,2,21,二者应该是JAVA和JavaScript的关系
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/807619506,David 9,719,2019-08-30T01:19:49.000Z,2019-08-30T01:32:00.000Z,0,1,SNN（脉冲神经网络）可能是更好的一种模拟方案：脉冲神经网络(SNN)会是下一代神经网络吗？ 知识梳理和源码解析，David 9的SNN初体验，Spiking Neural Network
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174637022,小学,543,2017-05-26T01:32:17.000Z,2017-05-26T01:32:17.000Z,0,6,几乎没有，神经网络这个词更像一种比喻。一个数学家搞出来的玩具，一群cs、EE方向的人玩的不亦乐乎……怎么看这些人都和生物没有半毛线关系。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/185094965,Kevin,98,2017-06-17T07:03:41.000Z,2017-06-17T07:03:41.000Z,0,5,"只能说是启发和影响，谈不上模拟。 早期的飞机受鸟类飞行启发，但说现代的飞机模拟鸟类飞行，也是不对的。 启发/灵感/影响主要有几个方面 1. 早期神经网络受生物大脑的神经元结构启发，逆向大脑来构建智能机器，但是一般都没有设计成生物功能的 2. 可以通过单一的深度学习算法解决不同的任务，比如把动物的视觉信号传送到听觉处理区域，动物可以学会用大脑的听觉处理区域去看（Von Melchner et al., 2000） 3. 受到动物视觉系统启发， 形成了现在的卷积神经网络的基础 4. 目前大多数神经网络使用的称为整流线性单元（rectified linear unit） 5. 连接主义的观点是，当网络将大量简单的计算单元连在一起可以实现智能，这也同样适用于生物神经系统中的神经元 深度网络或者神经网络无法模拟生物大脑 1. 没有足够关于大脑的知识作为指导 2. 现在计算和存储能力不足以支撑对大脑的模拟，比如同时监测（至少是）数千相连的神经元的活动 3. 更接近实际神经网络的系统并不能提高机器学习的性能 4. 有一些深度学习的研究人员引用神经网络作为参考，但是更多的人完全不关心神经科学 5. 实际的神经网络，比如前馈神经网络，更恰当的说法是为了实现泛化而设计的函数近似机 现代的神经网络更多地受数学和工程学科的指引，并且当前的神经网络或者深度学习的目标是构建计算机系统解决智能任务问题，而不是给大脑建模，它只是偶尔地神经科学提取灵感。 另外有一门学科《计算神经科学》，则是通过模拟大脑来了解大脑如何真实工作的。 做了一回搬运工，以上，参考 http://www.deeplearningbook.org/"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174645339,[已重置],274,2017-05-26T01:52:36.000Z,2017-05-26T02:17:31.000Z,1,2,"这方面我还没实际研究但想先建立讨论的共识什么叫做仿？至少有两个常见的代表涵义1.不是真的2.希望尽可能的真我用猜测的不是真的这点是肯定的但是否想尽可能的真我想这不是主要目的(就算是也是因为现状关于人类思维的传感器不够好)临时有任务回来再更音符除了音阶外重要的是时值而人脑可以选择无视-------------------------------------想说的主要是我们需要人工智慧、机器学习一类是功利性目的因此就算模仿人脑也是有限度的模仿因此就算能透过加随机数来决策失败也是模拟退火类的想跳脱视野侷限找出全域最大值但不作为、闹情绪、造假判定自己是否该解这题、装傻、忽悠、视而不见不是我们研究机器学习与人工智慧的目的的目的以这层面上来说确实不是模仿人脑但解题思路上多少会借鉴人脑最后推荐个TED视频「大脑存在的真正原因」(那视频出来时雷达传感器还不知名有些问题在那时来看很难解决!!)有很多生物都有大脑也有很多生物没有人脑的作用不同但借由观察其它生物为何需要大脑可以猜出人类有脑的主要原因而人类有些人能胜出的原因是大脑有附加价值其实具体原因在这题目前的最高顺位解答有写到人工智慧本身不处理传感器问题而大脑思考与传感器的相关性较计算机为高因此思考不是多数生物的强项此外「思考倾向于解释世界,而重点是改变世界」"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/543338717,卫龙大面筋,32,2018-12-01T22:09:37.000Z,2018-12-01T22:09:37.000Z,0,0,占个坑，明天来写
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/273879176,david,33,2017-12-10T07:01:11.000Z,2017-12-10T07:01:12.000Z,0,0,"首先，根据DNN的实际表现，应该和人脑的机制有部分重合，但是我觉得还是很初级的，DNN只是一个概况的称谓，还包括CNN,RNN....等不同的类型，可以理解为对应了人脑中不同功能的神经元结构。"
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/175803711,匿名用户,0,2017-05-28T16:56:57.000Z,2017-05-28T17:10:04.000Z,1,10,我猜来回答此问题的更多的为cs的专业人士。提问暗含的意思似乎是已经把单个神经元当作学习的最小功能单位，其实远非如此。一个神经细胞已经是无数分子和电信号整合后的反应大单位。甚至更精细一些，神经细胞上的一个突触都可以根据刺激强弱极为迅速地进行自我活性调控。而一小小突触，其下涉及的蛋白种类和瞬间的酶活应激变化何止千种。再给一百年，深度学习进化到再高的程度，恐怕也比不上单个神经元的plasticity能力。枉论是皮层了。概念上有相似之处，但这相似之处很牵强。因为人们并不了解皮层，所以其实本来也谈不上对皮层的概念，至多算是“模仿了脑神经领域中某个有争议的关于学习产生的简化理论模型”。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/174186533,匿名用户,0,2017-05-25T01:00:24.000Z,2017-05-25T01:00:24.000Z,8,4,最前沿的DNN网络有几百层就不错了。而人的神经细胞传播层数又有多少呢？况且人的神经细胞是以一个网络的形式，可能后一层的单元还会向前一层传播。目前的DNN只能说尽量去模拟，但真正的模拟，私以为是做不到的。一点愚见。
深度神经网络（DNN）是否模拟了人类大脑皮层结构？,https://www.zhihu.com/question/59800121/answer/239479836,匿名用户,0,2017-10-04T16:49:01.000Z,2017-10-04T16:49:01.000Z,11,3,先放结论：没有。学了一年人工智能，我怀疑现在人工智能的发展方向上存在着根本上的错误，人工神经网络的本质是n个超级复杂的函数堆砌，这个函数复杂到训练一个神经网络用我的小破MacBook pro要用一下午才能完成。直觉告诉我这种unexplainable Statistics Model是无法孕育出真正的智能的。想要创造出强Ai必须要从大脑机理上进行模拟才可以。所以身为一个以创造出强Ai为己任的超级富二代，我决定改行学生物了
